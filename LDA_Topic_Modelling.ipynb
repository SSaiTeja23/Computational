{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA Topic Modelling",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuhq-XC82lJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d9bc57bf-cbfb-4fcf-9987-b777958d2798"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT4dgrSp2sy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Phrases\n",
        "from gensim import corpora, models\n",
        "from nltk.tokenize import word_tokenize \n",
        "df=pd.read_csv(\"/content/sample_data/Final.csv\")\n",
        "tokens=[]\n",
        "bigram = Phrases()\n",
        "for a in df[\"cleaned_tweet\"]:\n",
        "  tokens.append(word_tokenize(a))\n",
        "bigram = models.Phrases(tokens, min_count=30)\n",
        "bigram_mod = models.phrases.Phraser(bigram)\n",
        "# trigram =models.Phrases(bigram[tokens])  \n",
        "# trigram_mod = models.phrases.Phraser(trigram)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvD-eXym29sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Top n-grams\n",
        "import operator\n",
        "dic={}\n",
        "count=0\n",
        "bigrams=[]\n",
        "for id in range(len(tokens)):\n",
        "    for token in bigram_mod[tokens[id]]:\n",
        "        if '_' in token:\n",
        "          bigrams.append(token)\n",
        "          tokens[id].append(token)\n",
        "for co in bigrams:\n",
        "  if co in dic:\n",
        "    dic[co]+=1\n",
        "  else:\n",
        "    dic[co]=1\n",
        "bi= sorted(dic.items(),key=operator.itemgetter(1),reverse=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO6sf9Lh4VZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "dba15aa1-05a6-415c-fd7a-161039a0c87d"
      },
      "source": [
        "bi[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('test_posit', 16045),\n",
              " ('dure_pandem', 14673),\n",
              " ('stay_home', 10218),\n",
              " ('via_youtub', 9095),\n",
              " ('social_distanc', 8541),\n",
              " ('confirm_case', 7021),\n",
              " ('dure_crisi', 6959),\n",
              " ('public_health', 6193),\n",
              " ('new_york', 5972),\n",
              " ('stay_safe', 5353)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xOjrTM8VMsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove rare and common tokens.\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(tokens)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=30, no_above=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsrBV_tI5egI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary Creation\n",
        "# id2word = corpora.Dictionary(bigram_mod[tokens])\n",
        "# Creating Bag-of-words\n",
        "# corpus = [id2word.doc2bow(text) for text in bigram_mod[tokens]]\n",
        "# print(corpus[:4])\n",
        "\n",
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yka-eMzGWCdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9d4d90ad-234e-4441-affe-0938b5362951"
      },
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 22212\n",
            "Number of documents: 997924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNsaSyBX5FeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9ffc42a0-4c41-48eb-eaf3-ba5f219ed950"
      },
      "source": [
        "# Train LDA mode\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=2000,\n",
        "                                           alpha='auto',\n",
        "                                           eta='auto',\n",
        "                                           iterations=400,\n",
        "                                           passes=20,\n",
        "                                           eval_every= None)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU1NSAPS5Yje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "461769dd-5fdd-48a0-9bf6-d17e11c91f4f"
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(lda_model.print_topics())\n",
        "#To print the % of topics a document is\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.074*\"case\" + 0.073*\"death\" + 0.073*\"new\" + 0.031*\"report\" + '\n",
            "  '0.031*\"number\" + 0.021*\"infect\" + 0.020*\"break\" + 0.018*\"counti\" + '\n",
            "  '0.017*\"confirm\" + 0.016*\"million\"'),\n",
            " (1,\n",
            "  '0.090*\"test\" + 0.038*\"patient\" + 0.034*\"news\" + 0.033*\"hospit\" + '\n",
            "  '0.031*\"posit\" + 0.022*\"doctor\" + 0.017*\"nurs\" + 0.016*\"decreas\" + '\n",
            "  '0.015*\"result\" + 0.015*\"test_posit\"'),\n",
            " (2,\n",
            "  '0.031*\"live\" + 0.028*\"today\" + 0.027*\"updat\" + 0.022*\"join\" + 0.019*\"dstv\" '\n",
            "  '+ 0.016*\"april\" + 0.016*\"data\" + 0.014*\"dr\" + 0.014*\"watch\" + '\n",
            "  '0.013*\"inform\"'),\n",
            " (3,\n",
            "  '0.051*\"home\" + 0.032*\"stay\" + 0.026*\"keep\" + 0.023*\"safe\" + '\n",
            "  '0.022*\"research\" + 0.011*\"anyon\" + 0.011*\"stayhom\" + 0.010*\"stay_home\" + '\n",
            "  '0.010*\"hand\" + 0.009*\"rais\"'),\n",
            " (4,\n",
            "  '0.037*\"thi\" + 0.023*\"ha\" + 0.019*\"peopl\" + 0.017*\"covid\" + 0.016*\"u\" + '\n",
            "  '0.013*\"get\" + 0.012*\"wa\" + 0.012*\"time\" + 0.010*\"one\" + 0.010*\"like\"'),\n",
            " (5,\n",
            "  '0.042*\"dure\" + 0.035*\"pandem\" + 0.027*\"help\" + 0.024*\"amp\" + 0.017*\"govern\" '\n",
            "  '+ 0.016*\"busi\" + 0.016*\"support\" + 0.015*\"work\" + 0.015*\"respons\" + '\n",
            "  '0.015*\"impact\"'),\n",
            " (6,\n",
            "  '0.053*\"via\" + 0.026*\"chang\" + 0.025*\"trump\" + 0.017*\"sign\" + 0.014*\"south\" '\n",
            "  '+ 0.014*\"australia\" + 0.013*\"nation\" + 0.013*\"global\" + 0.012*\"china\" + '\n",
            "  '0.012*\"american\"'),\n",
            " (7,\n",
            "  '0.047*\"health\" + 0.030*\"spread\" + 0.025*\"care\" + 0.025*\"worker\" + '\n",
            "  '0.018*\"school\" + 0.017*\"risk\" + 0.017*\"public\" + 0.017*\"protect\" + '\n",
            "  '0.017*\"social\" + 0.016*\"open\"'),\n",
            " (8,\n",
            "  '0.050*\"state\" + 0.024*\"citi\" + 0.021*\"youtub\" + 0.021*\"order\" + '\n",
            "  '0.019*\"restrict\" + 0.017*\"resid\" + 0.016*\"offici\" + 0.015*\"via_youtub\" + '\n",
            "  '0.012*\"toll\" + 0.011*\"depart\"'),\n",
            " (9,\n",
            "  '0.070*\"coronaviru\" + 0.036*\"use\" + 0.035*\"viru\" + 0.021*\"vaccin\" + '\n",
            "  '0.014*\"diseas\" + 0.014*\"africa\" + 0.014*\"studi\" + 0.014*\"effect\" + '\n",
            "  '0.013*\"treatment\" + 0.011*\"expert\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyw2ubculmgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}